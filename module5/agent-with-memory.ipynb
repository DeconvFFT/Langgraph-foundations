{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbfe8e1e",
   "metadata": {},
   "source": [
    "# Introduction to Langgraph Store\n",
    "- Provides a way to store and retrieve information across threads\n",
    "- Presistent Key-value store\n",
    "- When storing memories in Store, we provide:\n",
    "    - namespace: for the object, a tuple\n",
    "    - key\n",
    "    - value\n",
    "    - put to save object to the store by namespace and key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50d22560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "in_memory_store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33e1f36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Namespace for memory to save\n",
    "user_id = \"1\"\n",
    "namespace_for_memory = (user_id, \"memory\")\n",
    "\n",
    "# Save a memory to namespace as key and value\n",
    "key = str(uuid.uuid4())\n",
    "value = {\"food_preference\": \"I like pizza\"}\n",
    "\n",
    "# store in memory\n",
    "in_memory_store.put(namespace_for_memory, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ca0006c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Item(namespace=['1', 'memory'], key='d57b8449-45d8-499b-9076-a7e9a9f03cb2', value={'food_preference': 'I like pizza'}, created_at='2025-08-25T04:39:29.702217+00:00', updated_at='2025-08-25T04:39:29.702218+00:00', score=None)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for a memory\n",
    "memories = in_memory_store.search(namespace_for_memory)\n",
    "memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa64399f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d57b8449-45d8-499b-9076-a7e9a9f03cb2 {'food_preference': 'I like pizza'}\n"
     ]
    }
   ],
   "source": [
    "print(memories[0].key, memories[0].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c19e9ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'namespace': ['1', 'memory'],\n",
       " 'key': 'd57b8449-45d8-499b-9076-a7e9a9f03cb2',\n",
       " 'value': {'food_preference': 'I like pizza'},\n",
       " 'created_at': '2025-08-25T04:39:29.702217+00:00',\n",
       " 'updated_at': '2025-08-25T04:39:29.702218+00:00'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = in_memory_store.get(namespace_for_memory, key)\n",
    "memory.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000a1604",
   "metadata": {},
   "source": [
    "# Chatbot with longterm memory\n",
    "- Short term memory: Chatbot persists conversation history and/ or allow interruptions in chat\n",
    "- Long term memory: Chatbot can remember information about a specific user across all chat sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f95df2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-20b\", temperature=0)\n",
    "#llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22187e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello, John! 👋 How can I help you today?', additional_kwargs={'reasoning_content': 'User says \"hello I am John Doe!\" They want a greeting. Probably respond politely.'}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 77, 'total_tokens': 118, 'completion_time': 0.036494418, 'prompt_time': 0.005706051, 'queue_time': 0.195682218, 'total_time': 0.042200469}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_fd68d84dd3', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--80a5978e-0019-48fc-8efc-d92ec425d16d-0', usage_metadata={'input_tokens': 77, 'output_tokens': 41, 'total_tokens': 118})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"hello I am John Doe!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12b561aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "# chatbot instruction\n",
    "MODEL_SYSTEM_PROMPT = \"\"\" \n",
    "You are a helpful assistant with memory that provides information about user.\n",
    "If you have memory for this user, use it to personalize your responses.\n",
    "Here is the memory (it may be empty): {memory}\n",
    "\"\"\"\n",
    "\n",
    "# Create new memory from the chat history and any existing memory\n",
    "CREATE_MEMORY_PROMPT = \"\"\" \n",
    "You are collecting information about the user to personalize your responses.\n",
    "\n",
    "CURRENT USER INFORMATION:\n",
    "{memory}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Review the chat history below carefully.\n",
    "2. Identify new information about the user such as:\n",
    "    - Personal information (name, location, phone number, email, linkedin profile, github profile, portfolio, etc.)\n",
    "    - Professional Summary(1-2 sentences)\n",
    "    - Education (Degree, School, Graduation Year)\n",
    "    - Work Experience (jobtitle, company, dates, responsibilities, links)\n",
    "    - Publications (title, date, journal, link)\n",
    "    - Technical Skills (programming languages, frameworks, tools, any specific skills focused on things like (AI, Machine Learning, Data Science, etc.))\n",
    "    - Projects (title, description, skills, links)\n",
    "    - Preferences (likes, dislikes)\n",
    "    - Interests and hobbies\n",
    "    - Goals or future plans\n",
    "3. Merge any new information with the existing memory.\n",
    "4. Format the memory as clear, bulleted list.\n",
    "5. If the information conflicts with existing memory, keep the most recent version.\n",
    "\n",
    "Remember: Only include factual information directly stated by the user. Do not make assumptions or inferences\n",
    "\n",
    "Based on the chat history below, please update the user history.\n",
    "\"\"\"\n",
    "\n",
    "def call_model(state:MessagesState, config:RunnableConfig, store:BaseStore):\n",
    "    \"\"\" Load memory from store and personalize the chatbot's response \"\"\"\n",
    "    \n",
    "    # get user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    \n",
    "    # Retrieve memory from store\n",
    "    namespace = ('memory', user_id)\n",
    "    key = \"user_memory\"\n",
    "    existing_memory = store.get(namespace, key)\n",
    "    \n",
    "    # Extract the actual memory content if it exists and add a prefix\n",
    "    if existing_memory:\n",
    "        existing_memory_content = existing_memory.value.get('memory')\n",
    "    else:\n",
    "        existing_memory_content = \"No existing memory found\"\n",
    "    \n",
    "    # format the memory\n",
    "    system_message = MODEL_SYSTEM_PROMPT.format(memory=existing_memory_content)\n",
    "    \n",
    "    # respond using memory as well as the chat history\n",
    "    response = llm.invoke([SystemMessage(content=system_message)] + state['messages'])\n",
    "    \n",
    "    return {'messages': [response]}\n",
    "\n",
    "def write_memory(state:MessagesState, config:RunnableConfig, store:BaseStore):\n",
    "    \"\"\" Create new memory from the chat history and any existing memory \"\"\"\n",
    "    \n",
    "    # get user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    \n",
    "    # retrieve existing memory\n",
    "    namespace = ('memory', user_id)\n",
    "    key = \"user_memory\"\n",
    "    existing_memory = store.get(namespace, key)\n",
    "    \n",
    "    # Extract the actual memory content if it exists and add a prefix\n",
    "    if existing_memory:\n",
    "        existing_memory_content = existing_memory.value.get('memory')\n",
    "    else:\n",
    "        existing_memory_content = \"No existing memory found\"\n",
    "\n",
    "    system_message = CREATE_MEMORY_PROMPT.format(memory=existing_memory_content)\n",
    "    new_memory = llm.invoke([SystemMessage(content=system_message)] + state['messages'])\n",
    "    \n",
    "    store.put(namespace, key, {\"memory\": new_memory.content})   \n",
    "\n",
    "# build graph\n",
    "memory_builder = StateGraph(MessagesState)\n",
    "memory_builder.add_node(\"call_model\", call_model)\n",
    "memory_builder.add_node(\"create_memory\", write_memory)\n",
    "memory_builder.add_edge(START, \"call_model\")\n",
    "memory_builder.add_edge(\"call_model\", \"create_memory\")\n",
    "memory_builder.add_edge(\"create_memory\", END)\n",
    "\n",
    "# Store memory for Long term memory\n",
    "across_thread_memory = InMemoryStore()\n",
    "within_thread_memory = MemorySaver()\n",
    "memory_graph = memory_builder.compile(checkpointer = within_thread_memory,store = across_thread_memory)\n",
    "\n",
    "display(Image(memory_graph.get_graph().draw_mermaid_png()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e31484d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, my name is Saumya\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Saumya! 👋 How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"Hi, my name is Saumya\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in memory_graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "363fa07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I am a Senior Data Scientist\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Nice to meet you, Saumya! 👋 As a Senior Data Scientist, you’re probably juggling a mix of modeling, data engineering, and stakeholder communication. How can I support you today? Are you looking for help with a specific project, exploring new techniques, or maybe something else?\n"
     ]
    }
   ],
   "source": [
    "# User input \n",
    "input_messages = [HumanMessage(content=\"I am a Senior Data Scientist\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in memory_graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132750de",
   "metadata": {},
   "source": [
    "# Testing if the model can remember user profile from it's long term memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b2250e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you recommend I focus on for my resume?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Saumya!  \n",
      "As a Senior Data Scientist, your resume is your “first pitch” to recruiters and hiring managers.  Below are the key areas I’d recommend you focus on, along with a quick checklist so you can make sure each section hits the mark.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Executive Summary / Professional Profile  \n",
      "- **Keep it 2–3 lines.**  \n",
      "- Highlight **years of experience**, **core domains** (e.g., predictive modeling, NLP, computer vision), and a **quantifiable impact** (e.g., “boosted revenue by 12% through model‑driven pricing”).  \n",
      "- Mention any **leadership** or **cross‑functional** experience (e.g., “led a team of 5 data scientists on a multi‑year AI strategy”).\n",
      "\n",
      "> **Example**  \n",
      "> *Senior Data Scientist with 8+ years of experience in predictive analytics, machine learning, and AI strategy. Proven track record of delivering $3 M in incremental revenue by deploying end‑to‑end ML pipelines for e‑commerce and fintech clients.*\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Core Competencies / Technical Skills  \n",
      "- **Group by category** (Programming, ML Frameworks, Cloud, Databases, etc.).  \n",
      "- Use **bullet points** or a two‑column table for readability.  \n",
      "- Include **soft skills** that matter at senior level (e.g., “Team Leadership,” “Stakeholder Communication,” “Project Management”).\n",
      "\n",
      "> **Example**  \n",
      "> **Programming & Tools** – Python, R, SQL, Scala, Spark, Docker, Kubernetes  \n",
      "> **ML & AI** – TensorFlow, PyTorch, Scikit‑learn, XGBoost, LightGBM, AutoML  \n",
      "> **Cloud & Deployment** – AWS (SageMaker, Lambda), GCP, Azure, Terraform  \n",
      "> **Data Engineering** – Airflow, Kafka, Redshift, Snowflake  \n",
      "> **Soft Skills** – Cross‑functional Leadership, Agile Coaching, Technical Evangelism\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Professional Experience  \n",
      "### Format\n",
      "- **Company – Role – Dates**  \n",
      "- **One‑sentence impact statement** (quantified).  \n",
      "- **Bulleted achievements** (3–5 per role).  \n",
      "- **Use action verbs** and **metrics**.\n",
      "\n",
      "### What to Emphasize\n",
      "| Category | Why It Matters | Sample Bullet |\n",
      "|----------|----------------|---------------|\n",
      "| **Business Impact** | Shows ROI of your work | “Reduced churn by 18% by building a churn‑prediction model that informed targeted retention campaigns.” |\n",
      "| **Technical Depth** | Demonstrates mastery | “Designed a distributed deep‑learning pipeline that processed 10 TB of image data, cutting inference time from 12 s to 0.8 s.” |\n",
      "| **Leadership & Mentorship** | Senior roles need people skills | “Mentored 4 junior data scientists, leading to 2 promotions and a 30% increase in team productivity.” |\n",
      "| **Cross‑Functional Collaboration** | Highlights ability to work with product, engineering, and business | “Partnered with product managers to translate business KPIs into ML metrics, aligning model objectives with company goals.” |\n",
      "| **Innovation & Patents** | Sets you apart | “Co‑invented a novel anomaly‑detection algorithm now used in 3 production services.” |\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Projects & Publications (Optional but Powerful)  \n",
      "- **Showcase high‑impact, end‑to‑end projects** that demonstrate your full stack.  \n",
      "- Include **links** (GitHub, Kaggle, demo videos).  \n",
      "- If you have **publications or patents**, list them with DOI or patent number.\n",
      "\n",
      "> **Example**  \n",
      "> *“Author of ‘Real‑Time Fraud Detection with Graph Neural Networks’ – published in IEEE Transactions on Knowledge and Data Engineering, 2023.”*\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Education & Certifications  \n",
      "- **Degree(s)** – institution, year.  \n",
      "- **Relevant certifications** (AWS Certified Machine Learning – Specialty, TensorFlow Developer, etc.).  \n",
      "- **Continuing education** (Coursera, Udacity, etc.) if recent.\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Optional Sections  \n",
      "- **Awards & Recognitions** – e.g., “Data Science Hackathon Winner, 2022.”  \n",
      "- **Languages** – if you’re multilingual.  \n",
      "- **Volunteer / Consulting** – especially if it’s data‑science related.\n",
      "\n",
      "---\n",
      "\n",
      "## 7. Formatting & Style Tips  \n",
      "| Tip | Why It Helps |\n",
      "|-----|--------------|\n",
      "| **Keep it to 1–2 pages** | Recruiters skim fast. |\n",
      "| **Use a clean, professional template** | Avoid clutter; use white space. |\n",
      "| **Bullet points, not paragraphs** | Easier to scan. |\n",
      "| **Consistent tense** – past for previous roles, present for current. |\n",
      "| **Quantify everything** – percentages, dollar values, time saved, accuracy improvements. |\n",
      "| **Tailor to the job** – swap in keywords from the posting (e.g., “MLOps,” “Explainable AI”). |\n",
      "\n",
      "---\n",
      "\n",
      "## Quick Self‑Check  \n",
      "1. **Does each bullet start with a strong action verb?**  \n",
      "2. **Are there clear, quantifiable outcomes?**  \n",
      "3. **Is the resume free of jargon that only a data scientist would understand?**  \n",
      "4. **Have you highlighted leadership and mentorship?**  \n",
      "5. **Is the formatting consistent and easy to read?**\n",
      "\n",
      "---\n",
      "\n",
      "### Next Steps  \n",
      "- **Draft** a one‑page version focusing on the most recent role.  \n",
      "- **Ask a peer** to review for clarity and impact.  \n",
      "- **Save** in PDF and keep a clean Word/Google Doc for future edits.\n",
      "\n",
      "Feel free to share a draft or ask about specific sections—happy to dive deeper into any part!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"What do you recommend I focus on for my resume?\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in memory_graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c2adfa",
   "metadata": {},
   "source": [
    "# Trustcall\n",
    "- Helps with complex or hard to generate schemas\n",
    "- We don't have to generate schemas from scratch every time we want to update something in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23bfa1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trustcall import create_extractor\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# Schema \n",
    "class UserProfile(BaseModel):\n",
    "    \"\"\"User profile schema with typed fields\"\"\"\n",
    "    first_name: str = Field(description=\"The first name of the user\")\n",
    "    last_name: str = Field(description=\"The last name of the user\")\n",
    "    state: str = Field(description=\"The state of the user\")\n",
    "    email: str = Field(description=\"The email of the user\")\n",
    "    phone: str = Field(description=\"The phone number of the user\")\n",
    "    address: str = Field(description=\"The address of the user\")\n",
    "    city: str = Field(description=\"The city of the user\")\n",
    "    zip: str = Field(description=\"The zip code of the user\")\n",
    "    \n",
    "\n",
    "\n",
    "# Instruction\n",
    "system_msg = \"Extract the user profile from the following conversation\"\n",
    "\n",
    "# Invoke the extractor\n",
    "\n",
    "trust_call_extractor = create_extractor(\n",
    "    llm,\n",
    "    tools=[UserProfile],\n",
    "    tool_choice=\"UserProfile\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "52562de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[UserProfile(first_name='John', last_name='Doe', state='CA', email='john.doe@gmail.com', phone='', address='123 Main St', city='San Francisco', zip='94101')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message = \"Extract the user profile from the following conversation\"\n",
    "conversation = [HumanMessage(content=\"My name is John Doe and I live at 123 Main St, San Francisco, CA 94101\"),\n",
    "                HumanMessage(content=\"My email is john.doe@gmail.com\"),\n",
    "                ]\n",
    "result = trust_call_extractor.invoke({\"messages\": [SystemMessage(content=system_msg)]+conversation})\n",
    "schema = result['responses']\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9cde1734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the instruction\n",
    "updated_conversation = [HumanMessage(content=\"My name is John Doe and I live at 123 Main St, San Francisco, CA 94101\"),\n",
    "                HumanMessage(content=\"My email is john.doe@gmail.com\"),\n",
    "                HumanMessage(content=\"I recently moved to 2345 Broadway, Phoenix, AZ 85004\")]\n",
    "system_msg = f\"\"\"Update the memory (JSON doc) to incorporate new information from the following conversation\"\"\"\n",
    "\n",
    "# Invoke the extractor with the updated instruction and existing profile with the corresponding tool name (UserProfile)\n",
    "result = trust_call_extractor.invoke({\"messages\": [SystemMessage(content=system_msg)]+updated_conversation}, \n",
    "                                    {\"existing\": {\"UserProfile\": schema[0].model_dump()}})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ce42c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_name': 'John',\n",
       " 'last_name': 'Doe',\n",
       " 'state': 'AZ',\n",
       " 'email': 'john.doe@gmail.com',\n",
       " 'phone': '',\n",
       " 'address': '2345 Broadway',\n",
       " 'city': 'Phoenix',\n",
       " 'zip': '85004'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['responses'][0].model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa7cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chatbot instruction\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"You are a helpful assistant with memory that provides information about the user. \n",
    "If you have memory for this user, use it to personalize your responses.\n",
    "Here is the memory (it may be empty): {memory}\"\"\"\n",
    "\n",
    "# Extraction instruction\n",
    "TRUSTCALL_INSTRUCTION = \"\"\"Create or update the memory (JSON doc) to incorporate information from the following conversation:\"\"\"\n",
    "\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Load memory from the store and use it to personalize the chatbot's response.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Retrieve memory from the store\n",
    "    namespace = (\"memory\", user_id)\n",
    "    existing_memory = store.get(namespace, \"user_memory\")\n",
    "\n",
    "    # Format the memories for the system prompt\n",
    "    if existing_memory and existing_memory.value:\n",
    "        memory_dict = existing_memory.value\n",
    "        formatted_memory = (\n",
    "            f\"First Name: {memory_dict.get('first_name', 'Unknown')}\\n\"\n",
    "            f\"Last Name: {memory_dict.get('last_name', 'Unknown')}\\n\"\n",
    "            f\"Phone: {', '.join(memory_dict.get('phone', 'Unknown'))}\\n\"\n",
    "            f\"Email: {', '.join(memory_dict.get('email', 'Unknown'))}\\n\"\n",
    "            f\"Address: {', '.join(memory_dict.get('address', 'Unknown'))}\\n\"\n",
    "            f\"State: {', '.join(memory_dict.get('state', 'Unknown'))}\\n\" \n",
    "            f\"City: {', '.join(memory_dict.get('city', 'Unknown'))}\\n\"  \n",
    "            f\"Zip: {', '.join(memory_dict.get('zip', 'Unknown'))}\\n\"  \n",
    "        )\n",
    "    else:\n",
    "        formatted_memory = None\n",
    "\n",
    "    # Format the memory in the system prompt\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=formatted_memory)\n",
    "\n",
    "    # Respond using memory as well as the chat history\n",
    "    response = llm.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Reflect on the chat history and save a memory to the store.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Retrieve existing memory from the store\n",
    "    namespace = (\"memory\", user_id)\n",
    "    existing_memory = store.get(namespace, \"user_memory\")\n",
    "        \n",
    "    # Get the profile as the value from the list, and convert it to a JSON doc\n",
    "    existing_profile = {\"UserProfile\": existing_memory.value} if existing_memory else None\n",
    "    \n",
    "    # Invoke the extractor\n",
    "    result = trust_call_extractor.invoke({\"messages\": [SystemMessage(content=TRUSTCALL_INSTRUCTION)]+state[\"messages\"], \"existing\": existing_profile})\n",
    "    \n",
    "    # Get the updated profile as a JSON object\n",
    "    updated_profile = result[\"responses\"][0].model_dump()\n",
    "\n",
    "    # Save the updated profile\n",
    "    key = \"user_memory\"\n",
    "    store.put(namespace, key, updated_profile)\n",
    "\n",
    "# Define the graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_node(\"write_memory\", write_memory)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "builder.add_edge(\"call_model\", \"write_memory\")\n",
    "builder.add_edge(\"write_memory\", END)\n",
    "\n",
    "# Store for long-term (across-thread) memory\n",
    "across_thread_memory = InMemoryStore()\n",
    "\n",
    "# Checkpointer for short-term (within-thread) memory\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# Compile the graph with the checkpointer fir and store\n",
    "graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
